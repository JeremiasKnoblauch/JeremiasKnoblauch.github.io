[{"authors":null,"categories":null,"content":"I am currently Assistant Professor/Lecturer at UCL’s Department of Statistical Science. Until 07/2025, I am fully bought out of teaching and administrative duties with an EPSRC research fellowship to continue my work on Optimisation-centric Generalisations of Bayesian Inference. I am also a visiting researcher at the Alan Turing Institute’s Data-Centric Engineering Programme, as well as an advisor for HopStair and Idoven.\nMy research revolves around extending the paradigm of Bayesian inference to cope with the challenges posed by modern large-scale data, simulator models, and machine learning techniques. In this context, I am particularly interested in generalised Bayesian inference, model misspecification and robustification strategies, computational challenges involving intractability, and variational methods. If you would like to learn more about my research, I have summarised some of my key contributions in the following talk.\nPrior to that, I was a Biometrika Fellow in 2021/2022, also based at UCL. Before that, I was a doctoral candidate within the Oxford-Warwick Statistics programme (2016-2021) as well as the first UK-based Facebook Fellow (2020/2021). During that time, I also worked with the research arms of Amazon (2019) and DeepMind (2021). I remain eager to stay in close contact with industry, and am very open to being approached by potential industrial partners for both academic and non-academic work.\n  Download my (very likely outdated) resumé.\n","date":1688169600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1688169600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://academic-demo.netlify.app/author/jeremias-knoblauch/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jeremias-knoblauch/","section":"authors","summary":"I am currently Assistant Professor/Lecturer at UCL’s Department of Statistical Science. Until 07/2025, I am fully bought out of teaching and administrative duties with an EPSRC research fellowship to continue my work on Optimisation-centric Generalisations of Bayesian Inference.","tags":null,"title":"Jeremias Knoblauch","type":"authors"},{"authors":null,"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"https://academic-demo.netlify.app/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":["Veit Wild","Sahra Ghalebikesabi","Dino Sejdinovic","Jeremias Knoblauch"],"categories":null,"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"274174977fbef1dbe71442322012269f","permalink":"https://academic-demo.netlify.app/publication/wgf-gvi/","publishdate":"2023-01-07T00:00:00Z","relpermalink":"/publication/wgf-gvi/","section":"publication","summary":"We establish the first mathematically rigorous link between Bayesian, variational Bayesian, and ensemble methods. A key step towards this it to reformulate the non-convex optimisation problem typically encountered in deep learning as a convex optimisation in the space of probability measures. On a technical level, our contribution amounts to studying generalised variational inference through the lense of Wasserstein gradient flows. The result is a unified theory of various seemingly disconnected approaches that are commonly used for uncertainty quantification in deep learning -- including deep ensembles and (variational) Bayesian methods. This offers a fresh perspective on the reasons behind the success of deep ensembles over procedures based on parameterised variational inference, and allows the derivation of new ensembling schemes with convergence guarantees. We showcase this by proposing a family of interacting deep ensembles with direct parallels to the interactions of particle systems in thermodynamics, and use our theory to prove the convergence of these algorithms to a well-defined global minimiser on the space of probability measures.","tags":null,"title":"A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods","type":"publication"},{"authors":["Matias Altamirano","Francois-Xavier Briol","Jeremias Knoblauch"],"categories":null,"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"ff0a302a5f4ca95b678fa88a5870e034","permalink":"https://academic-demo.netlify.app/publication/rcgp/","publishdate":"2023-01-07T00:00:00Z","relpermalink":"/publication/rcgp/","section":"publication","summary":"To enable closed form conditioning, a common assumption in Gaussian process (GP) regression is independent and identically distributed Gaussian observation noise. This strong and simplistic assumption is often violated in practice, which leads to unreliable inferences and uncertainty quantification. Unfortunately, existing methods for robustifying GPs break closed-form conditioning, which makes them less attractive to practitioners and significantly more computationally expensive. In this paper, we demonstrate how to perform provably robust and conjugate Gaussian process (RCGP) regression at virtually no additional cost using generalised Bayesian inference. RCGP is particularly versatile as it enables exact conjugate closed form updates in all settings where standard GPs admit them. To demonstrate its strong empirical performance, we deploy RCGP for problems ranging from Bayesian optimisation to sparse variational Gaussian processes.","tags":null,"title":"Robust and Conjugate Gaussian Process Regression","type":"publication"},{"authors":["Matias Altamirano","Francois-Xavier Briol","Jeremias Knoblauch"],"categories":null,"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"3d700783446e0735c7e90cf69cd0ed8e","permalink":"https://academic-demo.netlify.app/publication/rccp/","publishdate":"2023-01-07T00:00:00Z","relpermalink":"/publication/rccp/","section":"publication","summary":"This paper proposes an online, provably robust, and scalable Bayesian approach for changepoint detection. The resulting algorithm has key advantages over previous work: it provides provable robustness by leveraging the generalised Bayesian perspective, and also addresses the scalability issues of previous attempts. Specifically, the proposed generalised Bayesian formalism leads to conjugate posteriors whose parameters are available in closed form by leveraging diffusion score matching. The resulting algorithm is exact, can be updated through simple algebra, and is more than 10 times faster than its closest competitor.","tags":null,"title":"Robust and Scalable Bayesian Online Changepoint Detection","type":"publication"},{"authors":[],"categories":null,"content":"","date":1686405600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686405600,"objectID":"e284fc9a351f7c9cb5a7062577057a5d","permalink":"https://academic-demo.netlify.app/talk/a-rigorous-link-between-deep-ensembles-and-variational-bayesian-methods/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/a-rigorous-link-between-deep-ensembles-and-variational-bayesian-methods/","section":"event","summary":"I establish a link between variational and Bayesian sampling schemes and deep Ensembles. In particular, both can be seen as implementations of the Wasserstein Gradient Flow; the former as the a regularised and the latter as an unregularised version.","tags":[],"title":"A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods","type":"event"},{"authors":["Takuo Matsubara","Jeremias Knoblauch","Francois-Xavier Briol","Chris Oates"],"categories":null,"content":"","date":1681516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681516800,"objectID":"63d16711f8a00de51330c9ff51d186e0","permalink":"https://academic-demo.netlify.app/publication/discrete-intractable-likelihood/","publishdate":"2023-04-15T00:00:00Z","relpermalink":"/publication/discrete-intractable-likelihood/","section":"publication","summary":"Discrete state spaces represent a major computational challenge to statistical inference, since the computation of normalization constants requires summation over large or possibly infinite sets, which can be impractical. This article addresses this computational challenge through the development of a novel generalized Bayesian inference procedure suitable for discrete intractable likelihood. Inspired by recent methodological advances for continuous data, the main idea is to update beliefs about model parameters using a discrete Fisher divergence, in lieu of the problematic intractable likelihood. The result is a generalized posterior that can be sampled from using standard computational tools, such as Markov chain Monte Carlo, circumventing the intractable normalizing constant. The statistical properties of the generalized posterior are analyzed, with sufficient conditions for posterior consistency and asymptotic normality established. In addition, a novel and general approach to calibration of generalized posteriors is proposed. Applications are presented on lattice models for discrete spatial data and on multivariate models for count data, where in each case the methodology facilitates generalized Bayesian inference at low computational cost. Supplementary materials for this article are available online.","tags":null,"title":"Generalized Bayesian Inference for Discrete Intractable Likelihood","type":"publication"},{"authors":["Miheer Dewaskar","Chris Tosh","Jeremias Knoblauch","David Dunson"],"categories":null,"content":"","date":1681516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681516800,"objectID":"6056adcc19bedb92f80390fa380da9f1","permalink":"https://academic-demo.netlify.app/publication/reweighted-likelihoods-tvd/","publishdate":"2023-04-15T00:00:00Z","relpermalink":"/publication/reweighted-likelihoods-tvd/","section":"publication","summary":"Likelihood-based inferences have been remarkably successful in wide-spanning application areas. However, even after due diligence in selecting a good model for the data at hand, there is inevitably some amount of model misspecification: outliers, data contamination or inappropriate parametric assumptions such as Gaussianity mean that most models are at best rough approximations of reality. A significant practical concern is that for certain inferences, even small amounts of model misspecification may have a substantial impact; a problem we refer to as brittleness. This article attempts to address the brittleness problem in likelihood-based inferences by choosing the most model friendly data generating process in a discrepancy-based neighbourhood of the empirical measure. This leads to a new Optimistically Weighted Likelihood (OWL), which robustifies the original likelihood by formally accounting for a small amount of model misspecification. Focusing on total variation (TV) neighborhoods, we study theoretical properties, develop inference algorithms and illustrate the methodology in applications to mixture models and regression.","tags":null,"title":"Robustifying likelihoods by optimistically re-weighting data","type":"publication"},{"authors":["Hisham Husain","Jeremias Knoblauch"],"categories":null,"content":"","date":1648252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648252800,"objectID":"012ea3ac095c6b41b832a33f2d84b1e7","permalink":"https://academic-demo.netlify.app/publication/gvi-duality/","publishdate":"2022-03-26T00:00:00Z","relpermalink":"/publication/gvi-duality/","section":"publication","summary":"We build on the optimization-centric view on Bayesian inference advocated by Knoblauch et al. (2019). Thinking about Bayesian and generalized Bayesian posteriors as the solutions to a regularized minimization problem allows us to answer an intriguing question: If minimization is the primal problem, then what is its dual?  By deriving the Fenchel dual of the problem, we demonstrate that this dual corresponds to an adversarial game:  In the dual space, the prior becomes the cost function for an adversary that seeks to perturb the likelihood [loss] function targeted by standard [generalized] Bayesian inference.  This implies that Bayes-like procedures are adversarially robust; providing another firm theoretical foundation for their empirical performance. Our contributions are foundational, and apply to a wide-ranging set of Machine Learning methods. This includes standard Bayesian inference, generalized Bayesian and Gibbs posteriors (Bissiri et al., 2016), as well as a diverse set of other methods including Generalized Variational Inference (Knoblauch et al., 2019) and the Wasserstein Autoencoder (Tolstikhin et al., 2017)","tags":null,"title":"Adversarial Interpretation of Bayesian Inference","type":"publication"},{"authors":["Charita Dellaporta","Jeremias Knoblauch","Theo Damoulas","Francois-Xavier Briol"],"categories":null,"content":"","date":1648252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648252800,"objectID":"dc20c0caaf1ce41d569b41403fcc4470","permalink":"https://academic-demo.netlify.app/publication/npl-mmd/","publishdate":"2022-03-26T00:00:00Z","relpermalink":"/publication/npl-mmd/","section":"publication","summary":"Simulator-based models are models for which the likelihood is intractable but simulation of synthetic data is possible. They are often used to describe complex real-world phenomena, and as such can often be misspecified in practice. Unfortunately, existing Bayesian approaches for simulators are known to perform poorly in those cases. In this paper, we propose a novel algorithm based on the posterior bootstrap and maximum mean discrepancy estimators. This leads to a highly-parallelisable Bayesian inference algorithm with strong robustness properties. This is demonstrated through an in-depth theoretical study which includes generalisation bounds and proofs of frequentist consistency and robustness of our posterior. The approach is then assessed on a range of examples including a g-and-k distribution and a toggle-switch model.","tags":null,"title":"Robust Bayesian Inference for Simulator-based Models via the MMD Posterior Bootstrap","type":"publication"},{"authors":null,"categories":null,"content":"Scope I am committed to advancing the foundations and methodology of optimisation-centric generalised Bayesian posteriors. My 3-year EPSRC fellowship will help me realise this vision by laying the necessary foundations. Over the course of this time, I will also establish my own research group on generalised Bayesian methods.\nBackground Arguably a fringe topic until even the early 1990s, Bayesian methods have become one of the most predominant statistical analysis frameworks in most of the sciences over the past two decades. Bayesian inference is attractive because data plays the role of updating one’s prior about the state of the world into a posterior belief. This has two advantages over other methods of statistical analysis: it allows for the incorporation of prior knowledge, and it provides a clear recipe for reconciling our prior beliefs with the true state of the world. While the standard Bayesian paradigm is conceptually elegant, operationalising it for statistical application requires at least three assumptions that are often unrealistic:\n  We need to pre-suppose that the true state of the world can be adequately described by an analytically available likelihood function indexed by some parameter—so that the degree of our prior knowledge about the real world is completely described by our prior knowledge about this parameter;\n  We have to assume that the statistical modeller is actually capable of explicitly formulating the full extent of her prior knowledge about this parameter in form of a probability distribution;\n  We need to surmise that the statistical modeller has enough computational budget so that the resulting Bayesian posterior belief can always be computed.\n  While these assumptions will never perfectly describe the reality of statistical analysis, they are a useful simplification within the classical paradigm of statistical analysis. In the classical paradigm, a domain expert with intimate knowledge of the natural phenomena that generated a data set D distills her expertise into a statistical model indexed by some parameter as well as a prior belief over reasonable values of said parameter. While this paradigm continues to adequately describe many scientific inquiries, the increasing availability of large scale data sets, computational power, and sophisticated black box models has given rise to a second way of performing statistical inference—the machine learning paradigm of statistical analysis. Where the classical paradigm was data-centric, the machine learning paradigm is model-centric: rather than seeking a parsimonious model that is hand-crafted to describe a single given data set D, models are instead intractable, overparameterised and often explicitly designed to adequately represent an inordinately large collection of possible data sets. In practice, this means that priors and models constructed in the machine learning paradigm are often determined before having knowledge about the nature of a data set to be analysed. Unfortunately, this violates assumptions 1 and 2 and leads to problems with misspecification. Beyond that, due to their aim of being near-universal black box modelling tools, statistical models specified in this way often are complex or intractable.\nAs a result, traditional Bayesian computation is infeasible, which leads to a violation of assumption 3. As a consequence, using standard Bayesian posteriors for inference within the machine learning paradigm is problematic for a number of practical reasons. These include a lack of robustness, ill-informed priors, and intractability. To address these problems, a number of generalised Bayes-like procedures have been proposed in the past: PAC-Bayes, Safe Bayes, and generalised belief updates. The optimisation-centric generalisation that I have introduced in my previous work recovers these pre-existing extensions, and axiomatically justifies a much larger class of generalised posterior distributions capable of addressing a variety of problems.\nUse Cases There is ample cause for further research on optimisation-centric generalised Bayesian methods: they reconcile the practices of modern large-scale inference with the assumptions underpinning Bayesian statistics. This has at least two significant practical advantages.\n  Enhancing Robustness: because it relies on assumptions 1 and 2, the standard Bayesian paradigm is not robust to poorly specified models or prior beliefs. Consequently, it results in misleading uncertainty quantification in various conditions that are frequently encountered in practice. This includes outliers, heterogeneous or contaminated data, and dependence of observations. Fortunately, generalised Bayesian posteriors overcome these issues and result in better uncertainty quantification.\n  Simplifying Computation: by virtue of its optimisation-centric nature, the generalisation studied as part of this fellowship can directly simplify computation relative to standard Bayesian posteriors. This has important …","date":1643587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643587200,"objectID":"2ec38a37e905f57b8abd546d8ce7c59a","permalink":"https://academic-demo.netlify.app/project/generalising-bayes/","publishdate":"2022-01-31T00:00:00Z","relpermalink":"/project/generalising-bayes/","section":"project","summary":"Bayesian methods require assumptions that are often misaligned with the realities of modern day inference. It is my conviction that we can overcome these challenges with an optimisation-centric generalisation of Bayes' rule.","tags":["Generalised Bayesian Inference","Robustness","Bayesian Machine Learning","Variational Inference"],"title":"Optimisation-centric generalisations of Bayesian inference","type":"project"},{"authors":["Joel Jaskari","Jaakko Sahlsten","Theodoros Damoulas","Jeremias Knoblauch","Simo Särkkä","Leo Kärkkäinen","Kustaa Hietala","Kimmo Kaski"],"categories":null,"content":"","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642809600,"objectID":"d85d3ac5fbe4d74ed6f1ce73502af8fb","permalink":"https://academic-demo.netlify.app/publication/bnn-retinopathy/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/publication/bnn-retinopathy/","section":"publication","summary":"Automatic classification of diabetic retinopathy from retinal images has been widely studied using deep neural networks with impressive results. However, there is a clinical need for estimation of the uncertainty in the classifications, a shortcoming of modern neural networks. Recently, approximate Bayesian deep learning methods have been proposed for the task but the studies have only considered the binary referable/non-referable diabetic retinopathy classification applied to benchmark datasets. We present novel results by systematically investigating a clinical dataset and a clinically relevant 5-class classification scheme, in addition to benchmark datasets and the binary classification scheme. Moreover, we derive a connection between uncertainty measures and classifier risk, from which we develop a new uncertainty measure. We observe that the previously proposed entropy-based uncertainty measure generalizes to the clinical dataset on the binary classification scheme but not on the 5-class scheme, whereas our new uncertainty measure generalizes to the latter case. ","tags":null,"title":"Uncertainty-aware deep learning methods for robust diabetic retinopathy classification","type":"publication"},{"authors":["Jeremias Knoblauch","Jack Jewson","Theodoros Damoulas"],"categories":null,"content":"","date":1639612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639612800,"objectID":"1b82fbda40de6bc6dd952f71dfee5eb2","permalink":"https://academic-demo.netlify.app/publication/gvi/","publishdate":"2021-12-16T00:00:00Z","relpermalink":"/publication/gvi/","section":"publication","summary":"We advocate an optimization-centric view of Bayesian inference. Our inspiration is therepresentation of Bayes’ rule as infinite-dimensional optimization (Csiszár, 1975; Donsker andVaradhan, 1975; Zellner, 1988). Equipped with this perspective, we study Bayesian inferencewhen one does not have access to (1) well-specified priors, (2) well-specified likelihoods, (3)infinite computing power. While these three assumptions underlie the standard Bayesian paradigm, they are typically inappropriate for modern Machine Learning applications. We propose addressing this through an optimization-centric generalization of Bayesian posteriors that we call the Rule of Three (RoT). The RoT can be justified axiomatically and recovers Bayesian, PAC-Bayesian and VI posteriors as special cases. While the RoT is primarily a conceptual and theoretical device, it also encompasses a novel sub-class of tractable posteriors which we call Generalized Variational Inference (GVI) posteriors. Just as the RoT, GVI posteriors are specified by three arguments: a loss, a divergence and a variationalfamily. They also possess a number of desirable properties, including modularity, Frequentistconsistency and an interpretation as approximate ELBO. We explore applications of GVI posteriors, and show that they can be used to improve robustness and posterior marginalson Bayesian Neural Networks and Deep Gaussian Processes.","tags":null,"title":"An Optimization-centric View on Bayes' Rule: Reviewing and Generalizing Variational Inference","type":"publication"},{"authors":["Takuo Matsubara","Jeremias Knoblauch","Francois-Xavier Briol","Chris Oates"],"categories":null,"content":"","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"e9e193d37a928a3afffd7b9959fd3110","permalink":"https://academic-demo.netlify.app/publication/robust_intractable_likelihood/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/publication/robust_intractable_likelihood/","section":"publication","summary":"Generalised Bayesian inference updates prior beliefs using a loss function, rather than a likelihood, and can therefore be used to confer robustness against possible misspecification of the likelihood. Here we consider generalised Bayesian inference with a Stein discrepancy as a loss function, motivated by applications in which the likelihood contains an intractable normalisation constant. In this context, the Stein discrepancy circumvents evaluation of the normalisation constant and produces generalised posteriors that are either closed form or accessible using standard Markov chain Monte Carlo. On a theoretical level, we show consistency, asymptotic normality, and bias-robustness of the generalised posterior, highlighting how these properties are impacted by the choice of Stein discrepancy. Then, we provide numerical experiments on a range of intractable distributions, including applications to kernel-based exponential family models and non-Gaussian graphical models.","tags":null,"title":"Robust generalised Bayesian inference for intractable likelihoods","type":"publication"},{"authors":[],"categories":null,"content":"","date":1616162400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616162400,"objectID":"1d524214d48d05fe41e6f10cc77854ad","permalink":"https://academic-demo.netlify.app/talk/optimisation-centric-generalisations-of-bayesian-inference/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/optimisation-centric-generalisations-of-bayesian-inference/","section":"event","summary":"An overview of the research I have done on generalised Bayesian methods 2019-2021","tags":[],"title":"Optimisation-centric generalisations of Bayesian inference","type":"event"},{"authors":["Juan Maronas","Oliver Hamelijnck","Jeremias Knoblauch","Theodoros Damoulas"],"categories":null,"content":"","date":1616025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616025600,"objectID":"b4f793006e2b556318adffba40577c03","permalink":"https://academic-demo.netlify.app/publication/tgp/","publishdate":"2021-03-18T00:00:00Z","relpermalink":"/publication/tgp/","section":"publication","summary":"Gaussian Processes (GP) can be used as flexible, non-parametric function priors. Inspired by the growing body of work on Normalizing Flows, we enlarge this class of priors through a parametric invertible transformation that can be made input-dependent. Doing so also allows us to encode interpretable prior knowledge (eg, boundedness constraints). We derive a variational approximation to the resulting Bayesian inference problem, which is as fast as stochastic variational GP regression (Hensman et al., 2013; Dezfouli and Bonilla, 2015). This makes the model a computationally efficient alternative to other hierarchical extensions of GP priors (Lázaro-Gredilla, 2012; Damianou and Lawrence, 2013). The resulting algorithm’s computational and inferential performance is excellent, and we demonstrate this on a range of data sets. For example, even with only 5 inducing points and an input-dependent flow, our method is consistently competitive with a standard sparse GP fitted using 100 inducing points.","tags":null,"title":"Transforming Gaussian processes with normalizing flows","type":"publication"},{"authors":["Sebastian Schmon","Patrick Cannon","Jeremias Knoblauch"],"categories":null,"content":"","date":1614038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614038400,"objectID":"477588868744d70574b8c2561cd11035","permalink":"https://academic-demo.netlify.app/publication/abc_genbayes/","publishdate":"2021-02-23T00:00:00Z","relpermalink":"/publication/abc_genbayes/","section":"publication","summary":"Complex simulators have become a ubiquitous tool in many scientific disciplines, providing high-fidelity, implicit probabilistic models of natural and social phenomena. Unfortunately, they typically lack the tractability required for conventional statistical analysis. Approximate Bayesian computation (ABC) has emerged as a key method in simulation-based inference, wherein the true model likelihood and posterior are approximated using samples from the simulator. In this paper, we draw connections between ABC and generalized Bayesian inference (GBI). First, we re-interpret the accept/reject step in ABC as an implicitly defined error model. We then argue that these implicit error models will invariably be misspecified. While ABC posteriors are often treated as a necessary evil for approximating the standard Bayesian posterior, this allows us to re-interpret ABC as a potential robustification strategy. This leads us to suggest the use of GBI within ABC, a use case we explore empirically. ","tags":null,"title":"Generalized Posteriors in Approximate Bayesian Computation","type":"publication"},{"authors":["Jeremias Knoblauch","吳恩達"],"categories":["Demo","教程"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://academic-demo.netlify.app/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":[],"categories":null,"content":"","date":1604930400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604930400,"objectID":"6958431916010c7ca2e474dd5b07c837","permalink":"https://academic-demo.netlify.app/talk/optimal-continual-learning-has-perfect-memory-and-is-np-hard/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/optimal-continual-learning-has-perfect-memory-and-is-np-hard/","section":"event","summary":"An overview of my ICML paper in 2020 which formally proves that Continual Learning without forgetting is computationally infeasible.","tags":[],"title":"Optimal Continual Learning has Perfect Memory and is NP-hard","type":"event"},{"authors":["Jeremias Knoblauch","Lara Vomfell"],"categories":null,"content":"","date":1603670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603670400,"objectID":"332536038a5f813b4bd4aacbc18667e4","permalink":"https://academic-demo.netlify.app/publication/tvd/","publishdate":"2020-10-26T00:00:00Z","relpermalink":"/publication/tvd/","section":"publication","summary":"Models of discrete-valued outcomes are easily misspecified if the data exhibit zero-inflation, overdispersion or contamination. Without additional knowledge about the existence and nature of this misspecification, model inference and prediction are adversely affected. Here, we introduce a robust discrepancy-based Bayesian approach using the Total Variation Distance (TVD). In the process, we address and resolve two challenges: First, we study convergence and robustness properties of a computationally efficient estimator for the TVD between a parametric model and the data-generating mechanism. Second, we provide an efficient inference method adapted from Lyddon et al. (2019) which corresponds to formulating an uninformative nonparametric prior directly over the data-generating mechanism. Lastly, we empirically demonstrate that our approach is robust and significantly improves predictive performance on a range of simulated and real world data.","tags":null,"title":"Robust Bayesian Inference for Discrete Outcomes with the Total Variation Distance","type":"publication"},{"authors":[],"categories":null,"content":"","date":1601388000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601388000,"objectID":"76cb382c290df6f5de298e66090e6786","permalink":"https://academic-demo.netlify.app/talk/generalised-variational-inference/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/generalised-variational-inference/","section":"event","summary":"An overview of Generalised Variational Inference, a family of methods to address robustness in Bayesian machine learning problems","tags":[],"title":"Generalised Variational Inference","type":"event"},{"authors":["Jeremias Knoblauch","Hisham Husain","Tom Diethe"],"categories":null,"content":"","date":1591660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591660800,"objectID":"0acaf8ef6106dc801dcd32734e5a99e8","permalink":"https://academic-demo.netlify.app/publication/ocl/","publishdate":"2020-09-06T00:00:00Z","relpermalink":"/publication/ocl/","section":"publication","summary":"Continual Learning (CL) algorithms incrementally learn a predictor or representation across multiple sequentially observed tasks. Designing CL algorithms that perform reliably and avoid so-called catastrophic forgetting has proven a persistent challenge. The current paper develops a theoretical approach that explains why. In particular, we derive the computational properties which CL algorithms would have to possess in order to avoid catastrophic forgetting. Our main finding is that such optimal CL algorithms generally solve an NP-hard problem and will require perfect memory to do so. The findings are of theoretical interest, but also explain the excellent performance of CL algorithms using experience replay, episodic memory and core sets relative to regularization-based approaches.","tags":null,"title":"Optimal Continual Learning has Perfect Memory and is NP-hard","type":"publication"},{"authors":["Jeremias Knoblauch"],"categories":null,"content":"","date":1571616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571616000,"objectID":"d2e120d4d53a4708d734ac1eab9624b5","permalink":"https://academic-demo.netlify.app/publication/frequentist_consistency_gvi/","publishdate":"2019-10-21T00:00:00Z","relpermalink":"/publication/frequentist_consistency_gvi/","section":"publication","summary":"This paper investigates Frequentist consistency properties of the posterior distributions constructed via Generalized Variational Inference (GVI). A number of generic and novel strategies are given for proving consistency, relying on the theory of Γ-convergence. Specifically, this paper shows that under minimal regularity conditions, the sequence of GVI posteriors is consistent and collapses to a point mass at the population-optimal parameter value as the number of observations goes to infinity. The results extend to the latent variable case without additional assumptions and hold under misspecification. Lastly, the paper explains how to apply the results to a selection of GVI posteriors with especially popular variational families. For example, consistency is established for GVI methods using the mean field normal variational family, normal mixtures, Gaussian process variational families as well as neural networks indexing a normal (mixture) distribution.","tags":null,"title":"Frequentist Consistency of Generalized Variational Inference","type":"publication"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you’ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head()  Charts Academic supports the popular Plotly chart format.\nSave your Plotly JSON in your page folder, for example chart.json, and then add the {{\u0026lt; chart data=\u0026#34;chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\n  (function() { let a = setInterval( function() { if ( typeof window.Plotly === \u0026#39;undefined\u0026#39; ) { return; } clearInterval( a ); Plotly.d3.json(\u0026#34;./line-chart.json\u0026#34;, function(chart) { Plotly.plot(\u0026#39;chart-841569732\u0026#39;, chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  You might also find the Plotly JSON Editor useful.\nMath Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}{n}) - \\nabla F(\\mathbf{x}{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$  renders as\n$$f(k;p_{0}^{}) = \\begin{cases}p_{0}^{} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables …","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://academic-demo.netlify.app/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Jeremias Knoblauch"],"categories":null,"content":"","date":1558396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558396800,"objectID":"6486a74f3ba0400db3726c7a8b8ccd5e","permalink":"https://academic-demo.netlify.app/publication/rgdp/","publishdate":"2019-05-21T00:00:00Z","relpermalink":"/publication/rgdp/","section":"publication","summary":"This report provides an in-depth overview over the implications and novelty Generalized Variational Inference (GVI) (Knoblauch et al., 2019) brings to Deep Gaussian Processes (DGPs) (Damianou \u0026 Lawrence, 2013). Specifically, robustness to model misspecification as well as principled alternatives for uncertainty quantification are motivated with an information-geometric view. These modifications have clear interpretations and can be implemented in less than 100 lines of Python code. Most importantly, the corresponding empirical results show that DGPs can greatly benefit from the presented enhancements.","tags":null,"title":"Robust Deep Gaussian Processes","type":"publication"},{"authors":["Jeremias Knoblauch"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;)     print(\u0026#34;Welcome to Academic!\u0026#34;)  Welcome to Academic!  Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post’s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://academic-demo.netlify.app/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}   Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://academic-demo.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Jeremias Knoblauch","Jack Jewson","Theodoros Damoulas"],"categories":null,"content":"","date":1525651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525651200,"objectID":"0276b3416f00fd6c7d5b17439455ba3e","permalink":"https://academic-demo.netlify.app/publication/rbocpd/","publishdate":"2018-05-07T00:00:00Z","relpermalink":"/publication/rbocpd/","section":"publication","summary":"We present the very first robust Bayesian Online Changepoint Detection algorithm through General Bayesian Inference (GBI) with β-divergences. The resulting inference procedure is doubly robust for both the predictive and the changepoint (CP) posterior, with linear time and constant space complexity. We provide a construction for exponential models and demonstrate it on the Bayesian Linear Regression model. In so doing, we make two additional contributions: Firstly, we make GBI scalable using Structural Variational approximations that are exact as β→0. Secondly, we give a principled way of choosing the divergence parameter β by minimizing expected predictive loss on-line. We offer the state of the art and improve the False Discovery Rate of CPs by more than 80% on real world data.","tags":null,"title":"Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with β-Divergences","type":"publication"},{"authors":["Jeremias Knoblauch","Theodoros Damoulas"],"categories":null,"content":"","date":1518134400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518134400,"objectID":"9a88a60d9b5e96abecf4aa4403fc8b0e","permalink":"https://academic-demo.netlify.app/publication/bocpdms/","publishdate":"2018-02-09T00:00:00Z","relpermalink":"/publication/bocpdms/","section":"publication","summary":"Bayesian On-line Changepoint Detection is extended to on-line model selection and non-stationary spatio-temporal processes. We propose spatially structured Vector Autoregressions (VARs) for modelling the process between changepoints (CPs) and give an upper bound on the approximation error of such models. The resulting algorithm performs prediction, model selection and CP detection on-line. Its time complexity is linear and its space complexity constant, and thus it is two orders of magnitudes faster than its closest competitor. In addition, it outperforms the state of the art for multivariate data.","tags":null,"title":"Spatio-Temporal Bayesian On-line Changepoint Detection with Model Selection","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://academic-demo.netlify.app/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]